LLMs & Inference Strategy

Providers
- OpenAI (o3, o4-mini, GPT-4.1): strong tool-use, broad ecosystem.
- Anthropic (Claude 3.7 Sonnet / Haiku): long context, safer outputs, great tool-use.
- Azure OpenAI: enterprise controls, regionalization.
- Groq: fast inference for Llama/llama3.1 + Mixtral variants.

Recommended Defaults
- General agentic/tool use: Claude 3.7 Sonnet, GPT-4.1, or o4-mini.
- Low-latency assistants: Claude 3.7 Haiku or smaller Llama3 variants on Groq.
- JSON/program synthesis: o4-mini or GPT-4.1.

Context Windows & Memory
- Keep short-term conversation memory (recent 10â€“20 messages) + rolling summaries.
- Store per-session summary in DB; reconstruct context on reconnect.
- Use RAG only when needed (e.g., project docs or org knowledge base).

Tool-Use Guidelines
- Design tool schemas with strict types and descriptive fields.
- Validate inputs server-side before execution; reject unsafe operations.
- Scope tools by org/project + user role; never over-expose.

Guardrails
- Enforce max-step/tool-call budgets per message.
- Rate-limit per org/user; circuit-break on repeated failures.
- Log tool calls with redaction (no secrets in prompts/metadata).

Prompting
- System prompt: role, goals, constraints, tool catalog + usage rules.
- Few-shot templates for common tasks (search, link requirements/tests, generate IDs).
- Always instruct models to respond with minimal JSON for tools; natural language otherwise.

Observability
- Track tokens, tool-call count, latency, success/failure reason.
- Sample sessions for quality review; store anonymized artifacts.

